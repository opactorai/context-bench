# ============================================
# Context-Code-Bench Environment Variables
# ============================================
# Benchmark suite for evaluating MCP documentation servers
# on AI agent framework integration tasks

# ============================================
# MCP SERVER API KEYS (REQUIRED)
# ============================================
# Three main MCP documentation servers tested in this benchmark

# NIA - Hybrid package search with documentation fallback
# Get it from: https://app.trynia.ai/api-keys
NIA_API_KEY=your_nia_api_key

# Context7 - Library-specific documentation retrieval
# Get it from: https://context7.com/dashboard
CONTEXT7_API_KEY=your_context7_api_key

# Deepcon - Deep context understanding across codebases
# Get it from: https://deepcon.ai/dashboard
DEEPCON_API_KEY=your_deepcon_api_key

# ============================================
# EVALUATION API KEY (REQUIRED)
# ============================================
# OpenRouter - Multi-model evaluation system
# Used for consensus-based evaluation with GPT-5, Grok-4, and Deepseek-v3.2
# Get it from: https://openrouter.ai/keys
OPENROUTER_API_KEY=your_openrouter_api_key

# ============================================
# OPTIONAL CONFIGURATION
# ============================================
# Customize evaluation models (defaults to GPT-5, Grok-4, Deepseek-v3.2)
# EVALUATOR_MODELS=openai/gpt-5,x-ai/grok-4,deepseek/deepseek-v3.2-exp
